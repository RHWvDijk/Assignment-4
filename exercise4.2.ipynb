{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Exercise 2\n",
    "# Group 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment the original script has been used. The *weights* parameter were set to 'imagenet' and *None*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_96 (Model)  (None, 3, 3, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1281      \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Found 144000 images belonging to 2 classes.\n",
      "Found 16000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - 48s 214ms/step - loss: 0.4793 - acc: 0.7957 - val_loss: 0.6579 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65785, saving model to Transfer model weights ImageNet_weights.hdf5\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 0.3640 - acc: 0.8390 - val_loss: 0.4958 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65785 to 0.49584, saving model to Transfer model weights ImageNet_weights.hdf5\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 42s 184ms/step - loss: 0.3092 - acc: 0.8700 - val_loss: 0.3060 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49584 to 0.30598, saving model to Transfer model weights ImageNet_weights.hdf5\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 0.2906 - acc: 0.8835 - val_loss: 0.3310 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30598\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 41s 184ms/step - loss: 0.2767 - acc: 0.8910 - val_loss: 0.4412 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30598\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 42s 184ms/step - loss: 0.2568 - acc: 0.8969 - val_loss: 0.7605 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30598\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 41s 184ms/step - loss: 0.2420 - acc: 0.9024 - val_loss: 0.2249 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30598 to 0.22487, saving model to Transfer model weights ImageNet_weights.hdf5\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 41s 184ms/step - loss: 0.2261 - acc: 0.9085 - val_loss: 0.2075 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22487 to 0.20751, saving model to Transfer model weights ImageNet_weights.hdf5\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 41s 184ms/step - loss: 0.2386 - acc: 0.9090 - val_loss: 0.2240 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20751\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 0.2341 - acc: 0.9026 - val_loss: 0.2648 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.20751\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "\n",
    "def get_pcam_generators(base_dir, train_batch_size=32, val_batch_size=32):\n",
    "\n",
    "     # dataset parameters\n",
    "     train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "     valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "\n",
    "     # instantiate data generators\n",
    "     datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "     train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=train_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=val_batch_size,\n",
    "                                             class_mode='binary')\n",
    "\n",
    "     return train_gen, val_gen\n",
    "\n",
    "# the size of the images in the PCAM dataset\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "\n",
    "input = Input(input_shape)\n",
    "\n",
    "# get the pretrained model, cut out the top layer\n",
    "pretrained = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# if the pretrained model it to be used as a feature extractor, and not for\n",
    "# fine-tuning, the weights of the model can be frozen in the following way\n",
    "# for layer in pretrained.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "output = pretrained(input)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input, output)\n",
    "\n",
    "# note the lower lr compared to the cnn example\n",
    "model.compile(SGD(lr=0.001, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print a summary of the model on screen\n",
    "model.summary()\n",
    "\n",
    "# get the data generators\n",
    "train_gen, val_gen = get_pcam_generators('C:/Users/Daniel/Documents/')\n",
    "\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'Transfer model weights ImageNet'\n",
    "model_filepath = model_name + '.json'\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model, note that we define \"mini-epochs\"\n",
    "train_steps = train_gen.n//train_gen.batch_size//20\n",
    "val_steps = val_gen.n//val_gen.batch_size//20\n",
    "\n",
    "# since the model is trained for only 10 \"mini-epochs\", i.e. half of the data is\n",
    "# not used during training\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10,\n",
    "                    callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
